{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn import metrics\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import *\nfrom keras.models import *\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.initializers import *\nfrom keras.optimizers import *\nimport keras.backend as K\nfrom keras.callbacks import *\nimport os\nimport time\nimport gc\nimport re\nimport random\nfrom nltk.tokenize import word_tokenize\nimport multiprocessing\n\n#设置随机种子保证可重复性\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\nseed_everything()",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e36f33823b7d494507097f43be79a35ae00b4d7a"
      },
      "cell_type": "code",
      "source": "puncts = ['-', '…', '*', '/', '=', '+', '\\\\', '^', '_', '²', '√', '|', '™', '£', '°', '₹', 'π']\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        if punct in x:\n            x = x.replace(punct, f' {punct} ')\n            \n    if '..' in x:\n        x = x.replace('..', '. . ')\n    if '2014. ' in x:\n        x = x.replace('2014. ', '2014 .  ')\n    if '2015. ' in x:\n        x = x.replace('2015. ', '2015 .  ')\n    if '2016. ' in x:\n        x = x.replace('2016. ', '2016 .  ')\n    if '.if ' in x:\n        x = x.replace('.if ', ' . if ')\n    if '2017. ' in x:\n        x = x.replace('2017. ', '2017 .  ')\n    if '2018. ' in x:\n        x = x.replace('2018. ', '2018 .  ')\n    if '2019. ' in x:\n        x = x.replace('2019. ', '2019 .  ')\n    if '2020. ' in x:\n        x = x.replace('2020. ', '2020 .  ')\n    if '\\u200b' in x:\n        x = x.replace('\\u200b', '')\n    if ' no.1 ' in x:\n        x = x.replace(' no.1', '1st')\n    for punct in [' .net',' react.js', ' vue.js']:\n            if punct in x:\n                x = x.replace(punct, ' asp.net ')\n    for punct in [' b.sc', ' m.sc', ' b.des', ' b.arch ', ' b.pharm', ' b.pharma']:\n            if punct in x:\n                x = x.replace(punct, ' b.tech ')\n    if ' amazon.in' in x:\n        x = x.replace(' amazon.in', ' amazon.com ')\n    if ' www.quora.com' in x:\n        x = x.replace(' www.quora.com', ' quora website ')\n    if ' quora.com' in x:\n        x = x.replace(' quora.com', ' quora website ')\n    if ' www.opham.main.quora.com' in x:\n        x = x.replace(' www.opham.main.quora.com', ' quora website ')\n    for punct in [ '.i ', '.what ', '.how ', '.is ', '.find ', '.why ', '.in ',]:\n            if punct in x:\n                x = x.replace(punct, '.'+' '+punct[1:]+' ')\n    return x",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e583cd301219d39b4b9f798949bca8be7b5082fa"
      },
      "cell_type": "code",
      "source": "misspell =[\n ('quorans', \"quoran\"), ('brexit', 'british exit from eu'), ('cryptocurrencies', \"cryptocurrency\"), \n ('redmi', \"huawei\"), (\"'the\", 'the'), \n ('coinbase', 'bitcoin base'), \n ('oneplus', \"huawei\"), (\"'i\", 'i'), ('uceed', \"Undergraduate common entrance examination for design\"), \n ('demonetisation', \"demonetization\"), ('bhakts', \"something with great influence\"), \n ('loy machedo', 'Personal Branding Strategist'),\n ('gdpr', \"general data protection regulation\"), \n ('yogi adityanath', 'current Chief Minister'), ('boruto', \"naruto ' s son\"), ('upwork', \"odesk\"), ('bnbr', 'moderation'),\n (\"'a\", 'a'), ('ali alshamsi', 'Entrepreneur'), ('dceu', 'american media franchise'),\n ('litecoin', 'bitcoin'), ('iiest', 'Indian Institute of Engineering Science and Technology'),\n ('unacademy', 'indian largest learning platform'), ('sjws', 'social justice warrior'), ('qoura', 'quora'), \n ('zerodha', 'Indian financial service company'), (\"qur'an\", 'quora'), \n ('tensorflow', \"keras\"), ('doklam', 'china indian border'), ('lnmiit', 'Institute of Information Technology'), \n ('gopal kavalireddi', 'Maverick'), ('muoet', 'entrance exam'), \n ('nicmar', 'National Institute of Construction Management and Research'),\n ('vajiram and ravi', 'institute for exam preparation'),\n ('adhaar', 'id'), ('zebpay', 'bitcoins'), ('elitmus', 'assessment and recruitment company'), ('srmjee', 'Joint Entrance Exam'),\n ('altcoins', 'bitcoin'), ('altcoin', \"bitcoin\"), ('hackerrank', 'code website'),\n ('awdhesh', 'Educator'), ('jiren', 'goku'), ('ryzen', 'intel'), ('baahubali', \"deadpool\"), ('koinex', 'bitcoin company'),\n ('mhcet', 'entrance exam'),\n (\"'no\", 'no'), ('binance', 'bitcoin'), ('byju', 'the learning app'),  ('srmjeee', 'entrance exam'), ('beerus', \"gogeta\"), \n \n ('sgsits', 'indian institute'), ('skripal', 'former russian military intelligence officer'), (\"'to\", 'to'), \n ('ftre', 'talent reward exam'), ('nanodegree', 'the certificate of the bachelor degree'),  ('gurugram', 'gurgaony'), \n ('hotstar', 'youtube'),  ('mhtcet', 'entrance exam'), (\"'you\", 'you'), (\"'white\", 'white'), ('bmsce', 'indian institute'), \n ('bipc', 'biology physics chemistry'), ('jiofi', \"wifi\"), \n (\"'not\", 'not'),  ('microservices', 'micro services'), ('swachh bharat', 'cleanliness campaign'), ('usict', 'indian college'), \n (\"'in\", 'in'), \n ('zenfone', 'vivo'), ('lbsnaa', 'research and training institute'),  ('clickbait', 'attention - grabbing headlines'), \n ('reactjs', 'javascript'),  ('patreon', 'kickstarter'),\n (\"y'all\", 'you all'), ('chromecast', 'ipod'), ('pessat', 'online exam'), ('bittrex', 'us - based bitcoin exchange'), \n ('sarahah', \"anonymous feedback tool\"), ('demonitisation', \"demonetization\"), \n ('jungkook', 'South Korean singer'), ('dream11', 'steam'),  ('iisers', 'indian college'), (\"'how\", 'how'), \n ('aktu', 'indian college'), ('bitconnect', \"cryptocurrency\"), \n ('kalpit veerwal', 'computer science sophomore'), ('deepmind', 'british artificial intelligence company'), \n (\"'good\", 'good'), (\"'all\", 'all'), ('aiats', 'All India Test Series'), (\"'my\", 'my'),  ('trumpcare', 'obamacare'), \n (\"'it\", 'it'), (\"'do\", 'do'), ('mmmut', 'University of Technology'), ('airpods', 'headphones'),\n ('xxxtentacion', 'american rapper'), \n ('hbtu', 'government technical university'), (\"'what\", 'what'), \n ('vssut', 'University of Technology'),  ('wannacry', 'ransomware worm'),  ('nlus', 'national law universities'),\n (\"'one\", 'one'),  ('rlwl', 'remote location waiting list'), (\"'r\", 'r'), ('onedrive', \"skydrive\"),  \n ('lnct', 'College of Technology'),\n ('codeforces', 'competitive programming contests website'), ('arrowverse', 'superhero'),\n (\"'free\", 'free'), ('despacito', \"song\"), ('fz25', 'bike type'), ('zamasu', 'gogeta'), \n ('electroneum', \"ethereum\"), \n ('irodov', 'physics'), (\"'why\", 'why'),  ('simpliv', 'coursera'),  \n ('iiith', 'International Institute of Information Technology'),  ('kovind', '14th President of India'),  \n ('eflu', 'English and Foreign Languages University'),\n ('internshala', 'coursera'), ('whydo', 'why do'), ('chapterwise', \"chapter wise\"),  \n ('ncerts', 'National Council of Educational Research and Training'),  ('genderfluid', 'intersex'),\n ('igdtuw', 'Technical University for Women'), \n ('ravindrababu', 'the online teacher'), ('₹', \"inr\"), ('twinflame', 'soulmate'), \n ('iiitd', 'Institute of Information Technology'), ('kubernetes', \"docker\"), \n  ('tissnet', 'national Entrance Test'), ('xiomi', \"xiaomi\"), ('blockchains', \"blockchain\"), \n  ('jcpoa', 'iran nuclear deal'), ('undergraduation', 'Undergraduate education'), ('incels', 'involuntary celibates'),\n ('overbrace', \"+ -\"),  ('schizoids', 'schizoid'), ('byjus', \"coursera\"), ('hackerearth', \"hackerspace\"), \n ('apist', \"rapist\"), (\"'new\", 'new'), (\"don'ts\", 'do not'), \n ('odoo', 'enterprise management system'), ('vitee', 'entrance exam'),  ('veerwal', 'computer science sophomore'), \n ('wikitribune', 'news platform'),\n (\"'friends\", 'friends'), (\"'if\", 'if'), \n ('ipmat', 'Integrated Program in Management Aptitude Test'), ('extc', 'electronics and telecommunication engineering'), \n ('dhinchak pooja', 'pop singer'), (\"''the\", 'the'), ('kaneki', 'comic character'),\n ('undertale', 'minecraft'), ('peter strzok', 'former united states federal bureau of investigation ( fbi ) agent'), \n ('padmaavat', 'queen'), (\"'real\", 'real'), ('sscbs', 'college of business studies'), ('yourquote', 'microblogging platform'), \n (\"'god\", 'god'), \n ('remainers', \"remain\"), ('pizzagate', \"debunked conspiracy theory\"), ('theranos', \" health technology corporation\"), \n ('drumpf', 'trump'), ('zhihu', \"chinese quora\"), ('makaut', \"college\"), (\"'x\", 'x'), \n \n (\"i'am\", 'i am'), ('qidian', \"chinese novel website\"), ('bmsit', \"Institute of Technology and Management\"), \n ('instacart', \"walmart\"), ('ailet', \"All India Law Entrance Test\"), (\"'normal\", 'normal'), \n ('lhmc', \"most authoritative dictionary database\"), (\"'yes\", 'yes'), (\"'get\", 'get'), \n ('mbappe', \"beckham\"), ('padmavat', \"queen\"), \n ('bitfinex', \"bitcoin platform\"), ('kainerugaba', \"Ugandan military officer\"), ('cos2x', 'cosx'), \n ('homepod', \"ipod\"), ('don´t', 'do not'), ('steemit', \"facebook\"), \n ('\\ufeff', ' '), ('jupyter', \"javascript\"), ('nsejs', \"National Standard Examination in Junior Science\"), \n ('doordash', \"walmart\"), ('msqe', \"Master of Science in Quantitative Economics\"), (\"'he\", 'he'), \n (\"'anti\", 'anti'), ('rgipt', \"nstitute of Petroleum Technology\"), ('2k17', '2017'), ('whyis', 'why is'), \n ('usaco', \"computing education\"), ('iihm', \"International Institute of Hotel Management\"), \n (\"'big\", 'big'), ('neuralink', \"spacex\"), (\"'black\", 'black'), (\"'quora\", 'quora'), ('i´m', 'i am'), (\"'fake\", 'fake'), \n ('taehyung', \"South Korean singer\"), (\"'best\", 'best'), ('pgdbf', \"future of Bank recruitment\"), \n ('upeseat', \"University of Petroleum & Energy Studies\"), \n (\"'we\", 'we'), ('mh370', \"Airlines Flight\"), \n ('openai', \"spacex\"), (\"'being\", 'being'), (\"'bad\", 'bad'), (\"'american\", 'american'),\n ('mobikwik', \"paytm\"), (\"'b\", 'b'), \n ('vitmee', \"Examination\"), ('aieea', \"All India Entrance Examination for Admission for under graduation\"), \n ('flipcart', \"flipkart\"), ('i`m', 'i am'), ('ubereats', \"ubder eats\"), (\"'hindu\", 'hindu'), \n ('plancess', \"edu solutions\"), ('brexiters', \"british exit from eu\"), ('kattankulathur', \"kanchipuram\"), \n \n (\"cat'17\", \"Common Admission Test 2017\"), ('demonitization', \"demonetization\"), ('killmonger', \"fictional supervillain\"), \n (\"'high\", 'high'), \n ('ipucet', \"University Common Entrance Test\"), \n ('ugee', \"touch bar\"), ('ipill', \"pill\"), ('gstin', \"identification\"), (\"'let\", \"let\"), (\"'go\", 'go'), ('tamilans', \"tamils\"), \n ('nluo', \"National Law University\"), ('segwit2x', \"cryptocurrency\"), ('unocoin', \"cryptocurrency\"), \n ('wumao', \"internet commentators\"), ('minance', \"financial institution\"), ('waymo', 'lyft'), \n (\"'friend\", 'friend'), ('covalency', \"valency\"), ('daesh', \"islamic state of iraq\"), \n ('nielit', \"National Institute of Electronics & Information Technology\"), ('aimcat', \"Test series\"), ('digitalocean', \"skydrive\"), \n ('ballb', \"bachelor\"), (\"'f\", \"f\"), (\"'just\", 'just'), ('webnovel', 'web novel'), ('2k18', '2018'), \n ('kefla', \"goku\"), ('niftem', \" National Institute Of Food Technology Entrepreneurship And Management\"), \n ('phonepe', \"iphone\"), (\"'time\", \"time\"), ('dogecoin', \"cryptocurrency\"), (\"'e\", \"e\"), ('musigma', \"mckinsey\"), \n ('jiophone', \"iphone\"), \n ('fitjee', \"tutorial\"), ('lrdi', \"Logical Reasoning and Data Interpretation\"), \n ('imucet', \"Exam\"), (\"'this\", \"this\"), ('zoomcar', \"lyft\"), ('deplorables', \" presidential election campaign speech\"), \n \n ('hypsm', \"college union\"), ('kilimall', \"amazon\"), ('tqwl', \" tatkal waiting list\"), (\"'non\", 'non'), \n (\"'is\", 'is'), (\"'p\", 'p'), ('sppu', \"University\"), \n (\"gov't\", 'gov'), ('can`t', 'can not'), ('jecrc', \"university\"), ('brexiteers', 'british exit from eu'), \n ('darkweb', \"dark web\"), (\"'c\", \"c\"), \n ('dsce', \"College of Engineering\"), ('alphago', \"deepblue\"), \n ('etoos', \"education\"), (\"'only\", 'only'), ('angular2', 'angular'), ('sanghis', \"sanghi\"),\n ('dilr', \"Data Interpretation and Logical Reasoning\"), ('trumpism', 'trump ism'), \n ('quoras', 'quoran'), ('onecoin', \"cryptocurrency\"), (\"'man\", \"man\"), ('graphql', 'graph sql'), \n (\"'she\", 'she'), ('autoencoder', \"auto encoder\"), ('arkit', \"ios tool\"), \n ('iert', \"Institute of Engineering and Rural Technology\"), (\"'right\", 'right'), (\"'baby\", 'baby'), \n ('toppr', \"learning app\"), ('practo', \"online doctor\"), ('hashflare', \"cryptocurrency mine\"), \n (\"'make\", 'make'), ('oliveboard', \"preparation platform\"), \n (\"'indian\", 'indian'), (\"'an\", 'an'), ('gdax', \"ethereum\"), ('narcissit', \"narcissism\"), \n ('rnsit', \"Institute of Technology\"), ('uppcs', \"Public Service Commission\"), \n ('poloniex', \"dollar stablecoin\"), ('whatapp', 'whatsapp'), ('simsree', \"Research And Entrepreneurship Education\"), \n ('siacoin', \"ethereum\"), (\"'happy\", 'happy'), (\"'out\", 'out'), ('bschools', \"school\"),\n ('sense8', \"science fiction drama web television series\"), \n ('antminer', \"ethereum\"), ('cringiest', \"cringy\"), ('rakshaks', \"rakshak\"), ('whydoes', 'why does'), \n (\"'t\", \"t\"), (\"'special\", 'special'), ('hololens', \"smart glasses\"), \n \n ('pytorch', \"keras\"), ('nearbuy', \"near buy\"), ('freecodecamp', \"freecode camp\"), ('strowman', \"wrestler\"), (\"'too\", \"too\"), \n ('gaslighted', \"gas lighted\"), ('mpstme', \"top Engineering colleges\"), ('hyperconjugation', \"no - bond resonance\"), \n ('gurmehar', \"left leaning Indian student activist and author\"), \n ('gujratis', \"gujrati\"), (\"''i\", 'i'), \n ('lenskart', \"flipkart\"), (\"'your\", 'your'), (\"'must\", 'must'), (\"'life\", 'life'), \n ('bstat', \"bachelor\"), (\"'on\", 'on'), \n (\"'flat\", 'flat'), (\"'more\", 'more'), (\"'as\", 'as'), (\"'so\", 'so'), ('thicc', \"fit\"), ('gaslighter', 'gas lighter'), \n ('techmahindra', \"infosys\"), ('cptsd', \"trauma\"), ('datacamp', \"data camp\"),  \n ('bajjika', \"french\"), (\"qu'ran\", 'quora'), (\"'great\", 'great'), ('baslp', \"bachelor\"), ('vuejs', 'javascript'), \n ('suryanamaskar', \"yogi\"), \n ('crytocurrency', \"cryptocurrency\"), (\"'science\", 'science'), ('bartetzko', \"soldier\"), (\"'red\", 'red'), \n ('iqoption', \"iq option\"), ('fnaf', \" media franchise\"), \n ('touchbar', \"touch bar\"), ('delhite', \"people in delhi\"), (\"'made\", 'made'), ('gitlab', 'github'), \n ('aayog', \"The National Institution for Transforming India\"), ('nofap', \"health platform\"), ('bftech', \"bachelor\"), \n (\"'hello\", 'hello'), (\"'who\", 'who'), ('sklearn', 'keras'), ('incel', \"alone\"), \n ('frdi', \"Financial Resolution and Deposit Insurance\"), \n (\"'that\", 'that'), ('howdoes', \"how does\"), ('etherum', \"ethereum\"), ('narcisists', \"narcissism\"), \n ('got7', 'got'), (\"'l\", 'l'), (\"'can\", 'can'), (\"'home\", \"home\"), ('neet2017', \"neet 2017\"),\n ('aimcats', 'exam'), ('alphazero', 'deepblue'), (\"'o\", 'o'), (\"'nothing\", 'nothing'), \n ('filecoin', \"cryptocurrency\"), ('madeeasy', \"made easy\"),\n(\"5'1\", \"5 feet 1 inch\"),\n(\"5'2\", \"5 feet 2 inch\"),\n(\"5'3\", \"5 feet 3 inch\"),\n(\"5'4\", \"5 feet 4 inch\"),\n(\"5'5\", \"5 feet 5 inch\"),\n(\"5'6\", \"5 feet 6 inch\"),\n(\"5'7\", \"5 feet 7 inch\"),\n(\"5'8\", \"5 feet 8 inch\"),\n(\"5'9\", \"5 feet 9 inch\"),\n(\"5'0\", \"5 feet 0 inch\"),\n(\"5'10\", \"5 feet 10 inch\"),\n(\"5'11\", \"5 feet 11 inch\"),\n(\"5'12\", \"5 feet 12 inch\"),\n(\"6'1\", \"6 feet 1 inch\"),\n(\"6'2\", \"6 feet 2 inch\"),\n(\"6'3\", \"6 feet 3 inch\"),\n(\"6'4\", \"6 feet 4 inch\"),\n(\"6'5\", \"6 feet 5 inch\"),\n(\"6'0\", \"6 feet 0 inch\"),]\nmispell_dict = {}\nfor words in misspell:\n    if 'why' in words[0] or 'how' in words[0]:\n        mispell_dict[words[0]] = words[1]+' '\n    mispell_dict[' '+words[0]] = ' '+words[1]+' '\n    \ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\n\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ebb29556b5088af0a5345d1ba1402bdf925f7e22"
      },
      "cell_type": "code",
      "source": "embed_size = 300 #嵌入词向量维度\nmax_features = None #词汇量\nmaxlen = 72 #样本长度\n\ndef preapre_data():\n    \n    #加载数据\n    train = pd.read_csv(\"../input/train.csv\")\n    test = pd.read_csv(\"../input/test.csv\")\n    \n    #文本处理（链接符分隔，词汇修正）\n    train[\"question_text\"] = train[\"question_text\"].str.lower()\n    test[\"question_text\"] = test[\"question_text\"].str.lower()\n    train[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_text(x))\n    test[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_text(x))\n    train[\"question_text\"] = train[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n    test[\"question_text\"] = test[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n    \n    \n    #缺失值处理\n    train[\"question_text\"] = train[\"question_text\"].fillna(\"_####_\")\n    test[\"question_text\"] = test[\"question_text\"].fillna(\"_####_\")\n    \n    with multiprocessing.Pool(2) as pool:\n        docs_tokenized_train = pool.map(word_tokenize, train['question_text'].values)\n        docs_tokenized_test = pool.map(word_tokenize, test['question_text'].values)\n\n    #句子分词\n    tokenizer = Tokenizer(num_words=max_features, filters='')\n    tokenizer.fit_on_texts(docs_tokenized_train + docs_tokenized_test)\n\n    #句子编码\n    X_all = tokenizer.texts_to_sequences(docs_tokenized_train)\n    X_test = tokenizer.texts_to_sequences(docs_tokenized_test)\n\n    #填充至一定长度，默认值为0\n    X_all = pad_sequences(X_all, maxlen=maxlen)\n    X_test = pad_sequences(X_test, maxlen=maxlen)\n\n    #样本标签\n    Y = train['target'].values\n\n    submission = test[['qid']]\n    return X_all, X_test, Y, tokenizer.word_index, submission\n\nX, X_test, Y, word_index, sub = preapre_data()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "42d960412d3e3296b11b7c6f04401a2333af9a9a"
      },
      "cell_type": "code",
      "source": "#嵌入矩阵长度\nmax_features = len(word_index)+1\ndef load_glove(word_index):\n    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n    def get_coefs(word, *arr): return word.lower(), np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if o.split(\" \")[0] in word_index)\n    emb_mean, emb_std = -0.005838499, 0.48782197\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n            \n    return embedding_matrix \n    \ndef load_para(word_index):\n    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n    def get_coefs(word, *arr): return word.lower(), np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100 and o.split(\" \")[0] in word_index)\n    emb_mean, emb_std = -0.0053247833, 0.49346462\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    \n    return embedding_matrix\n\ndef load_embedding_matrix():\n    seed_everything()\n    embedding_matrix_g = load_glove(word_index)\n    embedding_matrix_p = load_para(word_index)\n    return 0.6*embedding_matrix_g + 0.4*embedding_matrix_p, np.concatenate((embedding_matrix_g, embedding_matrix_p), axis=1)\n\nembedding_matrix_weight,  embedding_matrix_concat = load_embedding_matrix()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "60537403a3b1ed738ae5262951621f83f4f128d0"
      },
      "cell_type": "code",
      "source": "class AdamW(Optimizer):\n    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, weight_decay=1e-4,  # decoupled weight decay (1/4)\n                 epsilon=1e-8, decay=0., **kwargs):\n        super(AdamW, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.lr = K.variable(lr, name='lr')\n            self.beta_1 = K.variable(beta_1, name='beta_1')\n            self.beta_2 = K.variable(beta_2, name='beta_2')\n            self.decay = K.variable(decay, name='decay')\n            self.wd = K.variable(weight_decay, name='weight_decay') # decoupled weight decay (2/4)\n        self.epsilon = epsilon\n        self.initial_decay = decay\n\n    @interfaces.legacy_get_updates_support\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n        wd = self.wd # decoupled weight decay (3/4)\n\n        lr = self.lr\n        if self.initial_decay > 0:\n            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n                                                  K.dtype(self.decay))))\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n                     (1. - K.pow(self.beta_1, t)))\n\n        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        self.weights = [self.iterations] + ms + vs\n\n        for p, g, m, v in zip(params, grads, ms, vs):\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon) - lr * wd * p # decoupled weight decay (4/4)\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'beta_1': float(K.get_value(self.beta_1)),\n                  'beta_2': float(K.get_value(self.beta_2)),\n                  'decay': float(K.get_value(self.decay)),\n                  'weight_decay': float(K.get_value(self.wd)),\n                  'epsilon': self.epsilon}\n        base_config = super(AdamW, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7a855720285acbba575c2504ab34e8a3bce1e53a"
      },
      "cell_type": "code",
      "source": "#epoch=5\nembed_size = 600\ndef parallelRNN():\n    K.clear_session()\n    recurrent_units = 128\n    inp = Input(shape=(maxlen,))\n    embedding_layer = Embedding(max_features,\n                                embed_size,\n                                weights=[embedding_matrix_concat],\n                                input_length=maxlen,\n                                trainable=False)(inp)\n    embedding_layer = SpatialDropout1D(0.2, seed=10086)(embedding_layer)\n\n    gru = Bidirectional(CuDNNGRU(64, return_sequences=True, \n                                   kernel_initializer=glorot_uniform(seed=1008600), \n                                   recurrent_initializer=Orthogonal(gain=1.0, seed=1008600)))(embedding_layer)\n    lstm = Bidirectional(CuDNNLSTM(64, return_sequences=True,\n                                  kernel_initializer=glorot_uniform(seed=111000), \n                                  recurrent_initializer=Orthogonal(gain=1.0, seed=1008600)))(embedding_layer)\n    concat = concatenate([gru, lstm], axis=-1)\n    concat = GlobalMaxPooling1D()(concat)\n\n    output_layer = Dense(1, activation=\"sigmoid\", kernel_initializer=glorot_uniform(seed=10086))(concat)\n    model = Model(inputs=inp, outputs=output_layer)\n    model.compile(loss='binary_crossentropy', optimizer=AdamW(weight_decay=0.06))\n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c4371bb1919e953166955efbf109f36c110aa5e9"
      },
      "cell_type": "code",
      "source": "def f1_smart(y_true, y_pred):\n    args = np.argsort(y_pred)\n    tp = y_true.sum()\n    fs = (tp - np.cumsum(y_true[args[:-1]])) / np.arange(y_true.shape[0] + tp - 1, tp, -1)\n    res_idx = np.argmax(fs)\n    return 2 * fs[res_idx], (y_pred[args[res_idx]] + y_pred[args[res_idx + 1]]) / 2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fc3478809edb89717972efcc4605bcbd7fae1bd8"
      },
      "cell_type": "code",
      "source": "seed_everything()\nkfold = StratifiedKFold(n_splits=7, random_state=10, shuffle=True)\ny_test = np.zeros((X_test.shape[0], ))\nthresholds = [] \n\nfor i, (train_index, valid_index) in enumerate(kfold.split(X, Y)):\n    if i != 2 :continue\n    X_train, X_val, Y_train, Y_val = X[train_index], X[valid_index], Y[train_index], Y[valid_index]\n    filepath=\"weights_best.h5\"\n    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0, verbose=2)\n    callbacks = [checkpoint, reduce_lr]\n    model = parallelRNN()\n    model.fit(X_train, Y_train, batch_size=512, epochs=7, validation_data=(X_val, Y_val), verbose=2, \n              callbacks=callbacks, shuffle=False, class_weight={0:1, 1:1.25}\n             )\n    model.load_weights(filepath)\n    y_pred = model.predict([X_val], batch_size=1024, verbose=1)\n    y_test = np.squeeze(model.predict([X_test], batch_size=1024, verbose=1))\n    f1, threshold = f1_smart(np.squeeze(Y_val), np.squeeze(y_pred))\n    thresholds.append(threshold)\n    print('Optimal F1: {:.4f} at threshold: {:.4f}'.format(f1, threshold))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8dfeaa6cf4313400f274d1701575fe41e89b1f87"
      },
      "cell_type": "code",
      "source": "y_test = y_test.reshape((-1, 1))\npred_test_y = (y_test>np.mean(thresholds)).astype(int)\nsub['prediction'] = pred_test_y\nsub.to_csv(\"submission.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "34380f58b1587f21686ad3d0e996c7e680eecade"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}