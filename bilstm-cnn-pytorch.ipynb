{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import re\nimport time\nimport gc\nimport random\nimport os\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import f1_score, roc_auc_score\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.functional as F\nfrom torch.optim.optimizer import Optimizer\nfrom tqdm import tqdm\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def seed_torch(seed=1029):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85ee6b26944da2cd972c5038fa63c78b763184d3"},"cell_type":"code","source":"embed_size = 300 # how big is each word vector\nmax_features = 200000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 72 # max number of words in a question to use\n\nbatch_size = 512\ntrain_epochs = 6\n\nSEED = 1029\nseed_torch(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5013e3945f2454854337ff3ee7eee98efa79fd25"},"cell_type":"code","source":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        if punct in x:\n            x = x.replace(punct, f' {punct} ')\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87a5fb67cdd061388568a1cd27250cecf5fb24a7"},"cell_type":"code","source":"def load_and_prec():\n    train_df = pd.read_csv(\"../input/train.csv\")\n    test_df = pd.read_csv(\"../input/test.csv\")\n    print(\"Train shape : \",train_df.shape)\n    print(\"Test shape : \",test_df.shape)\n    \n    # lower\n    train_df[\"question_text\"] = train_df[\"question_text\"].str.lower()\n    test_df[\"question_text\"] = test_df[\"question_text\"].str.lower()\n    \n    # Clean the text\n    train_df[\"question_text\"] = train_df[\"question_text\"].apply(clean_text)\n    test_df[\"question_text\"] = test_df[\"question_text\"].apply(clean_text)\n        \n    ## fill up the missing values\n    train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n    test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n\n    ## Tokenize the sentences\n    tokenizer = Tokenizer(num_words=max_features, filters='')\n    tokenizer.fit_on_texts(list(train_X)+list(test_X))\n    train_X = tokenizer.texts_to_sequences(train_X)\n    test_X = tokenizer.texts_to_sequences(test_X)\n\n    ## Pad the sentences \n    train_X = pad_sequences(train_X, maxlen=maxlen)\n    test_X = pad_sequences(test_X, maxlen=maxlen)\n\n    ## Get the target values\n    train_y = train_df['target'].values\n    \n    #shuffling the data\n    np.random.seed(SEED)\n    trn_idx = np.random.permutation(len(train_X))\n\n    train_X = train_X[trn_idx]\n    train_y = train_y[trn_idx]\n    sub = test_df[['qid']]\n    return train_X, test_X, train_y, tokenizer.word_index, sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9fa4b3016d2edf2c14340de1f23988b086850851"},"cell_type":"code","source":"def load_glove(word_index):\n    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) \n                            for o in open(EMBEDDING_FILE) \n                            if o.split(\" \")[0] in word_index or o.split(\" \")[0].lower() in word_index)\n    emb_mean, emb_std = -0.005838499, 0.48782197\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: \n            embedding_matrix[i] = embedding_vector\n        elif embeddings_index.get(word.lower()) is not None:\n            embedding_matrix[i] = embeddings_index.get(word.lower())\n            \n    return embedding_matrix \n    \ndef load_para(word_index):\n    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) \n                            for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') \n                            if len(o)>100 and (o.split(\" \")[0] in word_index or o.split(\" \")[0].lower() in word_index))\n    emb_mean, emb_std = -0.0053247833, 0.49346462\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: \n            embedding_matrix[i] = embedding_vector\n        elif embeddings_index.get(word.lower()) is not None:\n            embedding_matrix[i] = embeddings_index.get(word.lower())\n    \n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06944041d5cad5617c641b73f513c4840eb502fc"},"cell_type":"code","source":"%%time\nseed_torch(1026)\nX, X_test, Y, word_index, sub = load_and_prec()\nembedding_matrix_1 = load_glove(word_index)\nembedding_matrix_2 = load_para(word_index)\n\nembedding_matrix = np.mean([embedding_matrix_1, embedding_matrix_2], axis=0)\nprint(np.shape(embedding_matrix))\ndel embedding_matrix_1, embedding_matrix_2\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71eeb24155b28b82e08b89becf8ba2cf3e207d67"},"cell_type":"code","source":"class LSTM_CNN(nn.Module):\n    def __init__(self):\n        super(LSTM_CNN, self).__init__()\n        \n        torch.cuda.empty_cache()\n        self.embedding = nn.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1])\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.kernel_nums = [96, 72, 48, 20]\n        self.kernel_sizes = [1, 2, 3, 5]\n        self.embedding_dropout = nn.Dropout2d(0.22)\n        self.lstm = nn.LSTM(embed_size, 128, bidirectional=True, batch_first=True)\n        #self.gru = nn.GRU(256, 64, bidirectional=True, batch_first=True)\n        self.convs = nn.ModuleList(\n            [nn.Conv1d(256, kn, ks)\n             for kn, ks in zip(self.kernel_nums, self.kernel_sizes)])\n        self.conbns = nn.ModuleList([nn.BatchNorm1d(i) for i in self.kernel_nums])\n        #self.gru_attention = Attention(128, maxlen)\n        \n        self.linear = nn.Linear(sum(self.kernel_nums), 100)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.22)\n        self.bn = nn.BatchNorm1d(100)\n        self.out = nn.Linear(100, 1)\n        \n    def forward(self, x):\n        h_embedding = self.embedding(x)\n        h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n        \n        h_lstm ,_ = self.lstm(h_embedding)        \n        ps = []\n        x = h_lstm.permute(0,2,1)\n        for i,conv in enumerate(self.convs):\n            c = conv(x)  # [b,e,msl]->[b,h,msl-k]\n            c = self.conbns[i](c)\n#             \n            c = F.relu(c)\n            p = F.max_pool1d(c, kernel_size=c.size(-1)).squeeze(-1)  # [b,h]\n            ps.append(p)\n        \n        conc = torch.cat(ps, 1)#240\n        conc = self.relu(self.linear(conc))#(240, 100)\n        conc = self.dropout(conc)#0.22\n        conc = self.bn(conc)\n        out = self.out(conc)#(100, 1)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6c179e5478f1ac92b69020fb875bf3d8154bff3"},"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef f1_smart(y_true, y_pred):\n    args = np.argsort(y_pred)\n    tp = y_true.sum()\n    fs = (tp - np.cumsum(y_true[args[:-1]])) / np.arange(y_true.shape[0] + tp - 1, tp, -1)\n    res_idx = np.argmax(fs)\n    return 2 * fs[res_idx], (y_pred[args[res_idx]] + y_pred[args[res_idx + 1]]) / 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"905eff5f26c9aba2a9b169011b1b51ed66aa7115"},"cell_type":"code","source":"from torch.optim.optimizer import Optimizer\n\nclass AdamW(Optimizer):\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0, amsgrad=False):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay, amsgrad=amsgrad)\n        super(AdamW, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(AdamW, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('amsgrad', False)\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n                amsgrad = group['amsgrad']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                if amsgrad:\n                    max_exp_avg_sq = state['max_exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                # if group['weight_decay'] != 0:\n                #     grad = grad.add(group['weight_decay'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                if amsgrad:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n\n                bias_correction1 = 1 - beta1 ** state['step']\n                bias_correction2 = 1 - beta2 ** state['step']\n                step_size = group['lr'] * np.sqrt(bias_correction2) / bias_correction1\n\n                # p.data.addcdiv_(-step_size, exp_avg, denom)\n                p.data.add_(-step_size,  torch.mul(p.data, group['weight_decay']).addcdiv_(1, exp_avg, denom) )\n\n        return loss\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f80aa8c099cb76eede9c0c5db859a835fc9d1f9"},"cell_type":"code","source":"# matrix for the out-of-fold predictions\ntrain_preds = np.zeros((len(X)))\n# matrix for the predictions on the test set\ntest_preds = np.zeros((len(X_test)))\nbestscore = []\n# always call this before training for deterministic results\nseed_torch()\n\nx_test_cuda = torch.tensor(X_test, dtype=torch.long).cuda()\ntest = torch.utils.data.TensorDataset(x_test_cuda)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\nkfold = StratifiedKFold(n_splits=5, random_state=10, shuffle=True)\nfor i, (train_idx, valid_idx) in enumerate(kfold.split(X, Y)):    \n    # split data in train / validation according to the KFold indeces\n    # also, convert them to a torch tensor and store them on the GPU (done with .cuda())\n    x_train_fold = torch.tensor(X[train_idx], dtype=torch.long).cuda()\n    y_train_fold = torch.tensor(Y[train_idx, np.newaxis], dtype=torch.float32).cuda()\n    x_val_fold = torch.tensor(X[valid_idx], dtype=torch.long).cuda()\n    y_val_fold = torch.tensor(Y[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n    \n    model = LSTM_CNN()\n    # make sure everything in the model is running on the GPU\n    model.cuda()\n\n    # define binary cross entropy loss\n    # note that the model returns logit to take advantage of the log-sum-exp trick \n    # for numerical stability in the loss\n    #loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum', pos_weight=torch.cuda.FloatTensor([1.25]))\n    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')\n    optimizer = AdamW(model.parameters(), weight_decay = 0.06)\n\n    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n    \n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n    \n    print(f'Fold {i + 1}')\n    \n    for epoch in range(4):\n        # set train mode of the model. This enables operations which are only applied during training like dropout\n        start_time = time.time()\n        model.train()\n        avg_loss = 0.  \n        for x_batch, y_batch in tqdm(train_loader, disable=True):\n            # Forward pass: compute predicted y by passing x to the model.\n            y_pred = model(x_batch)\n\n            # Compute and print loss.\n            loss = loss_fn(y_pred, y_batch)\n\n            # Before the backward pass, use the optimizer object to zero all of the\n            # gradients for the Tensors it will update (which are the learnable weights\n            # of the model)\n            optimizer.zero_grad()\n\n            # Backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n\n            # Calling the step function on an Optimizer makes an update to its parameters\n            optimizer.step()\n            avg_loss += loss.item() / len(train_idx)\n        # set evaluation mode of the model. This disabled operations which are only applied during training like dropout\n        model.eval()\n        \n        # predict all the samples in y_val_fold batch per batch\n        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n        test_preds_fold = np.zeros((len(X_test)))\n        \n        avg_val_loss = 0.\n        for j, (x_batch, y_batch) in enumerate(valid_loader):\n            y_pred = model(x_batch).detach()\n            \n            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_idx)\n            valid_preds_fold[j * batch_size:(j+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n        \n        elapsed_time = time.time() - start_time \n        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n            epoch + 1, 4, avg_loss, avg_val_loss, elapsed_time))\n        \n    # predict all samples in the test set batch per batch\n    for j, (x_batch,) in enumerate(test_loader):\n        y_pred = model(x_batch).detach()\n\n        test_preds_fold[j * batch_size:(j+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n\n    train_preds[valid_idx] = valid_preds_fold\n    test_preds += test_preds_fold / 5\n    f1, threshold = f1_smart(np.squeeze(Y[valid_idx]), np.squeeze(valid_preds_fold))\n    print('Optimal F1: {:.4f} at threshold: {:.4f}'.format(f1, threshold))\n    bestscore.append(threshold)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0c7655686bd19130f7f2c8074b2ec44b1451848"},"cell_type":"code","source":"f1, threshold = f1_smart(np.squeeze(Y), np.squeeze(train_preds))\nprint('Optimal F1: {:.4f} at threshold: {:.4f}'.format(f1, threshold))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3581f74ae694eb07182e2a23c9db8f01a78f1ba"},"cell_type":"code","source":"y_test = test_preds.reshape((-1, 1))\npred_test_y = (y_test>threshold).astype(int)\nsub['prediction'] = pred_test_y\nsub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"956b62fa62b97a506ff2accc3c018ad8cb3eef93"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}